# @package _global_

defaults:
  - override /trainer: default.yaml
  - override /model: randla_net_model.yaml
  - override /datamodule: datamodule.yaml
  - override /callbacks: callbacks.yaml
  - override /logger: comet

seed: 12345

hydra:
  verbose: false
fit_the_model: true
test_the_model: false

logger:
  comet:
    experiment_name: "V2.1 - Reproduce SS25000"

trainer:
  num_sanity_val_steps: 1
  log_every_n_steps: 1
  min_epochs: 200
  max_epochs: 1000
  # check_val_every_n_epoch: 2
  val_check_interval: 0.25 # batches/steps
  gpus: 0

model:
  model_architecture: "randla_net"
  loss: "CrossEntropyLoss"
  alpha: 0.25
  lr: 0.001
  n_classes: 2
  reduce_lr_on_plateau:
    activate: true
    factor: 0.5  # halving
    patience: 5
    cooldown: 2
  net:
    d_in: 10  # 3 + F
    num_classes: 2
    num_neighbors: 16 
    decimation: 4  # divide by decimation for each of the 4 local encoder.

early_stopping:
  _target_: pytorch_lightning.callbacks.EarlyStopping
  monitor: "val/loss_epoch" # name of the logged metric which determines when model is improving
  mode: "min" # can be "max" or "min"
  patience: 10 # how many **validation** epochs of not improving until training stops
  min_delta: 0 # minimum change in the monitored metric needed to qualify as an improvement

datamodule:
  limit_top_k_tiles_train: 10000000
  limit_top_k_tiles_val: 100000000

  train_subtiles_by_tile: 12 # Large to have single tile chosen.
  batch_size: 64 # Larger possible for faster training.
  
  subsample_size: 25000  # Aim for at least 10pts/mÂ²

