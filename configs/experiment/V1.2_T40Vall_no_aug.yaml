# @package _global_

defaults:
  - override /trainer: default.yaml
  - override /model: point_net_model.yaml
  - override /datamodule: datamodule.yaml
  - override /callbacks: callbacks.yaml
  - override /logger: comet

seed: 12345

hydra:
  verbose: false
fit_the_model: true
test_the_model: false

trainer:
  num_sanity_val_steps: 1
  log_every_n_steps: 1
  min_epochs: 1
  max_epochs: 80
  check_val_every_n_epoch: 2
  gpus: 0

callbacks:
  early_stopping:
    patience: 10  # patience * check_val_every_n_epoch = nb of training epochs before stopping.
  model_checkpoint:
    filename: "T40Vall_epoch_{epoch:03d}"

logger:
  comet:
    experiment_name: T40Vall_no_aug

model:
  loss: "FocalLoss"
  n_classes: 2
  alpha: 0.25
  net:
    MLP1_channels: [9, 64, 64]
    MLP2_channels: [64, 256, 512, 1024]
    MLP3_channels: [1088, 512, 256, 64, 4]
    batch_norm: true
    pi_init: 0.05

datamodule:
  augment: false
  limit_top_k_tiles_train: 40
  limit_top_k_tiles_val: 100000000

  train_subtiles_by_tile: 12 # Large to have single tile chosen.
  batch_size: 32 # Large possible for faster training.
  
  subsample_size: 25000  # Aim for at least 10pts/mÂ²

  num_workers: 4
